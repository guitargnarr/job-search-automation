#!/usr/bin/env python3
"""
Job Search Automation Platform - Dry Run Test Suite
Tests the new automation system without making actual API calls
"""

import sys
import os
import json
import asyncio
from pathlib import Path
from datetime import datetime
import importlib.util
import ast

print("="*70)
print("JOB SEARCH AUTOMATION PLATFORM - DRY RUN TEST SUITE")
print("="*70)
print("Testing without database connections or API calls...")
print()

test_results = {
    "timestamp": datetime.now().isoformat(),
    "mode": "DRY_RUN",
    "passed": [],
    "failed": [],
    "warnings": [],
    "modules_tested": {}
}

# TEST 1: Python Environment
print("\n[TEST 1] Python Environment")
print("-" * 70)
print(f"Python version: {sys.version}")
print(f"Working directory: {os.getcwd()}")
required_version = (3, 8)
current_version = sys.version_info[:2]
if current_version >= required_version:
    print(f"  ‚úì Python {current_version[0]}.{current_version[1]} meets requirement (>= 3.8)")
    test_results["passed"].append("Python version check")
else:
    print(f"  ‚úó Python {current_version[0]}.{current_version[1]} below requirement (>= 3.8)")
    test_results["failed"].append("Python version check")

# TEST 2: Backend Structure
print("\n[TEST 2] Backend Structure Validation")
print("-" * 70)

backend_structure = {
    "backend/": "Backend root directory",
    "backend/api/": "API endpoints",
    "backend/api/v1/": "API version 1",
    "backend/api/v1/email.py": "Email endpoints",
    "backend/api/v1/ats.py": "ATS endpoints",
    "backend/api/v1/linkedin.py": "LinkedIn endpoints",
    "backend/api/v1/applications.py": "Application endpoints",
    "backend/api/v1/jobs.py": "Jobs endpoints",
    "backend/api/v1/analytics.py": "Analytics endpoints",
    "backend/api/v1/follow_ups.py": "Follow-up endpoints",
    "backend/services/": "Service layer",
    "backend/services/email_service.py": "Email service",
    "backend/services/ats_optimizer.py": "ATS optimizer",
    "backend/services/linkedin_service.py": "LinkedIn automation",
    "backend/models/": "Database models",
    "backend/models/models.py": "SQLAlchemy models",
    "backend/core/": "Core utilities",
    "backend/core/database.py": "Database connection",
    "backend/core/config.py": "Configuration",
    "backend/core/logging.py": "Logging setup",
    "backend/main.py": "FastAPI app"
}

for path, description in backend_structure.items():
    full_path = Path(path)
    if full_path.exists():
        if full_path.is_file():
            size_kb = full_path.stat().st_size / 1024
            print(f"  ‚úì {path} ({size_kb:.1f} KB) - {description}")

            # Validate Python syntax for .py files
            if path.endswith('.py'):
                try:
                    with open(full_path, 'r') as f:
                        ast.parse(f.read())
                    test_results["passed"].append(f"Syntax: {path}")
                except SyntaxError as e:
                    print(f"    ‚úó Syntax error in {path}: {e}")
                    test_results["failed"].append(f"Syntax: {path}")
        else:
            print(f"  ‚úì {path} - {description}")
        test_results["passed"].append(f"Structure: {path}")
    else:
        print(f"  ‚úó {path} - Missing")
        test_results["failed"].append(f"Missing: {path}")

# TEST 3: Module Import Testing (Dry Run)
print("\n[TEST 3] Module Import Testing (Dry Run)")
print("-" * 70)

modules_to_test = [
    ("backend.models.models", "Database Models"),
    ("backend.services.email_service", "Email Service"),
    ("backend.services.ats_optimizer", "ATS Optimizer"),
    ("backend.services.linkedin_service", "LinkedIn Service"),
    ("backend.api.v1.email", "Email API"),
    ("backend.api.v1.ats", "ATS API"),
    ("backend.api.v1.linkedin", "LinkedIn API"),
    ("backend.api.v1.applications", "Applications API"),
    ("backend.api.v1.jobs", "Jobs API"),
    ("backend.api.v1.analytics", "Analytics API"),
    ("backend.api.v1.follow_ups", "Follow-ups API")
]

print("  Checking if modules can be imported (syntax validation)...")
for module_name, description in modules_to_test:
    try:
        # Try to find the module spec without importing
        module_path = module_name.replace('.', '/') + '.py'
        if Path(module_path).exists():
            print(f"  ‚úì {module_name} - {description} (found)")
            test_results["modules_tested"][module_name] = "found"
        else:
            print(f"  ‚ö† {module_name} - {description} (not found)")
            test_results["warnings"].append(f"Module not found: {module_name}")
    except Exception as e:
        print(f"  ‚úó {module_name} - Error: {e}")
        test_results["failed"].append(f"Module check: {module_name}")

# TEST 4: API Endpoint Definitions
print("\n[TEST 4] API Endpoint Definitions (Static Analysis)")
print("-" * 70)

api_files = [
    "backend/api/v1/email.py",
    "backend/api/v1/ats.py",
    "backend/api/v1/linkedin.py",
    "backend/api/v1/applications.py",
    "backend/api/v1/jobs.py",
    "backend/api/v1/analytics.py",
    "backend/api/v1/follow_ups.py"
]

total_endpoints = 0
for api_file in api_files:
    if Path(api_file).exists():
        try:
            with open(api_file, 'r') as f:
                content = f.read()
                # Count route decorators
                routes = content.count('@router.')
                total_endpoints += routes
                print(f"  ‚úì {Path(api_file).name}: {routes} endpoints defined")
                test_results["passed"].append(f"Endpoints: {api_file}")
        except Exception as e:
            print(f"  ‚úó {Path(api_file).name}: Error reading - {e}")
            test_results["failed"].append(f"Endpoints: {api_file}")

print(f"\n  Total endpoints defined: {total_endpoints}")

# TEST 5: Configuration Files
print("\n[TEST 5] Configuration Files")
print("-" * 70)

config_files = {
    ".env.example": "Environment template",
    "setup.py": "Setup script",
    "run.sh": "Run script",
    "requirements.txt": "Python dependencies",
    "README_AUTOMATION.md": "Documentation"
}

for config_file, description in config_files.items():
    if Path(config_file).exists():
        size_kb = Path(config_file).stat().st_size / 1024
        print(f"  ‚úì {config_file} ({size_kb:.1f} KB) - {description}")
        test_results["passed"].append(f"Config: {config_file}")
    else:
        print(f"  ‚úó {config_file} - Missing {description}")
        test_results["warnings"].append(f"Missing config: {config_file}")

# TEST 6: Service Layer Analysis
print("\n[TEST 6] Service Layer Analysis (Dry Run)")
print("-" * 70)

service_files = [
    ("backend/services/email_service.py", ["EmailService", "scan_inbox", "get_responses"]),
    ("backend/services/ats_optimizer.py", ["ATSOptimizer", "analyze_job_description", "optimize_resume"]),
    ("backend/services/linkedin_service.py", ["LinkedInService", "search_employees", "send_connection"])
]

for service_file, expected_components in service_files:
    if Path(service_file).exists():
        try:
            with open(service_file, 'r') as f:
                content = f.read()
                found = []
                for component in expected_components:
                    if component in content:
                        found.append(component)

                print(f"  ‚úì {Path(service_file).name}:")
                print(f"    Components found: {', '.join(found)}")
                if len(found) == len(expected_components):
                    test_results["passed"].append(f"Service complete: {service_file}")
                else:
                    missing = set(expected_components) - set(found)
                    print(f"    Missing: {', '.join(missing)}")
                    test_results["warnings"].append(f"Incomplete service: {service_file}")
        except Exception as e:
            print(f"  ‚úó {Path(service_file).name}: Error - {e}")
            test_results["failed"].append(f"Service error: {service_file}")

# TEST 7: Database Models Validation
print("\n[TEST 7] Database Models Validation (Dry Run)")
print("-" * 70)

if Path("backend/models/models.py").exists():
    try:
        with open("backend/models/models.py", 'r') as f:
            content = f.read()

        # Check for essential models
        models = ["Application", "Job", "Company", "EmailResponse", "LinkedInConnection", "FollowUp"]
        found_models = []

        for model in models:
            if f"class {model}" in content:
                found_models.append(model)

        print(f"  ‚úì Models defined: {len(found_models)}")
        for model in found_models:
            print(f"    ‚Ä¢ {model}")

        if len(found_models) == len(models):
            test_results["passed"].append("All models defined")
        else:
            missing = set(models) - set(found_models)
            print(f"  ‚ö† Missing models: {', '.join(missing)}")
            test_results["warnings"].append(f"Missing models: {', '.join(missing)}")

    except Exception as e:
        print(f"  ‚úó Error reading models: {e}")
        test_results["failed"].append("Models validation")

# TEST 8: Dry Run API Simulation
print("\n[TEST 8] API Endpoint Simulation (Dry Run)")
print("-" * 70)

print("  Simulating API calls without network requests...")

simulated_endpoints = [
    ("POST /api/v1/email/scan", "Would scan Gmail inbox"),
    ("GET /api/v1/email/responses", "Would fetch email responses"),
    ("POST /api/v1/ats/optimize-resume", "Would optimize resume for ATS"),
    ("POST /api/v1/linkedin/campaign", "Would run LinkedIn campaign"),
    ("GET /api/v1/applications/stats", "Would get application statistics"),
    ("GET /api/v1/analytics/dashboard", "Would fetch dashboard data"),
    ("GET /api/v1/follow-ups/scheduled", "Would get scheduled follow-ups")
]

for endpoint, description in simulated_endpoints:
    print(f"  [DRY RUN] {endpoint}")
    print(f"    ‚Üí {description}")
    test_results["passed"].append(f"Dry run: {endpoint}")

# TEST 9: Dependencies Check
print("\n[TEST 9] Python Dependencies Check")
print("-" * 70)

required_packages = [
    "fastapi",
    "sqlalchemy",
    "pydantic",
    "python-dotenv",
    "aiofiles",
    "httpx"
]

print("  Checking if packages are importable...")
for package in required_packages:
    try:
        __import__(package)
        print(f"  ‚úì {package} - Installed")
        test_results["passed"].append(f"Dependency: {package}")
    except ImportError:
        print(f"  ‚úó {package} - Not installed (run: pip install {package})")
        test_results["warnings"].append(f"Missing package: {package}")

# SUMMARY
print("\n" + "="*70)
print("DRY RUN TEST SUMMARY")
print("="*70)
print(f"‚úì Passed:   {len(test_results['passed'])}")
print(f"‚úó Failed:   {len(test_results['failed'])}")
print(f"‚ö† Warnings: {len(test_results['warnings'])}")
print("="*70)

# Performance metrics (simulated)
print("\n[SIMULATED PERFORMANCE METRICS]")
print("-" * 70)
print("  Email scan time: ~2.5 seconds (simulated)")
print("  ATS optimization: ~1.8 seconds (simulated)")
print("  LinkedIn search: ~3.2 seconds (simulated)")
print("  Dashboard load: ~0.5 seconds (simulated)")

# Save results
test_results["summary"] = {
    "passed": len(test_results["passed"]),
    "failed": len(test_results["failed"]),
    "warnings": len(test_results["warnings"]),
    "total_tests": len(test_results["passed"]) + len(test_results["failed"])
}

with open('test_results_dry_run.json', 'w') as f:
    json.dump(test_results, f, indent=2)

print(f"\nüìä Detailed results saved to test_results_dry_run.json")

# Show failures if any
if test_results["failed"]:
    print("\n‚ùå FAILED TESTS:")
    for fail in test_results["failed"]:
        print(f"  - {fail}")

# Show warnings if any
if test_results["warnings"]:
    print("\n‚ö†Ô∏è  WARNINGS:")
    for warn in test_results["warnings"]:
        print(f"  - {warn}")

# Overall status
if len(test_results["failed"]) == 0:
    print("\n‚úÖ DRY RUN COMPLETED SUCCESSFULLY!")
    print("   All static checks passed")
    print("   The automation platform structure is valid")

    if len(test_results["warnings"]) > 0:
        print("\n   Next steps:")
        print("   1. Install missing dependencies: pip install -r requirements.txt")
        print("   2. Run setup.py to initialize the system")
        print("   3. Configure .env with your credentials")
        print("   4. Run ./run.sh to start the server")
else:
    print("\n‚ùå DRY RUN DETECTED ISSUES")
    print("   Please fix the failed tests above")

sys.exit(0 if len(test_results["failed"]) == 0 else 1)